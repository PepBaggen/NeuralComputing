{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf1be1b2f55b62c0",
   "metadata": {},
   "source": [
    "# Food Classification with CNN - Building a Restaurant Recommendation System\n",
    "\n",
    "This assignment focuses on developing a deep learning-based food classification system using Convolutional Neural Networks (CNNs). You will build a model that can recognize different food categories and use it to return the food preferences of a user.\n",
    "\n",
    "## Learning Objectives\n",
    "- Implement CNNs for image classification\n",
    "- Work with real-world food image datasets\n",
    "- Build a preference-detector system\n",
    "\n",
    "## Background: AI-Powered Food Preference Discovery\n",
    "\n",
    "The system's core idea is simple:\n",
    "\n",
    "1. Users upload 10 photos of dishes they enjoy\n",
    "2. Your CNN classifies these images into the 91 categories\n",
    "3. Based on these categories, the system returns the user's taste profile\n",
    "\n",
    "Your task is to develop the core computer vision component that will power this detection engine.\n",
    "\n",
    "You are given a training (\"train\" folder) and a test (\"test\" folder) dataset which have ~45k and ~22k samples respectively. For each one of the 91 classes there is a subdirectory containing the images of the respective class.\n",
    "\n",
    "## Assignment Requirements\n",
    "\n",
    "### Technical Requirements\n",
    "- Implement your own pytorch CNN architecture for food image classification\n",
    "- Use only the provided training dataset split for training\n",
    "- Train the network from scratch ; No pretrained weights can be used\n",
    "- Report test-accuracy after every epoch\n",
    "- Report all hyperparameters of final model\n",
    "- Use a fixed seed and do not use any CUDA-features that break reproducibility\n",
    "- Use Pytorch 2.6\n",
    "\n",
    "### Deliverables\n",
    "1. Jupyter Notebook with CNN implementation, training code etc.\n",
    "2. README file\n",
    "3. Report (max 3 pages)\n",
    "\n",
    "Submit your report, README and all code files as a single zip file named GROUP_[number]_NC2425_PA. The names and IDs of the group components must be mentioned in the README.\n",
    "Do not include the dataset in your submission.\n",
    "\n",
    "### Grading\n",
    "\n",
    "1. Correct CNN implementation, training runs on the uni computers (computer rooms DM.0.07, DM.0.13, DM.0.17, DM.0.21) according to the README.MD instructions without ANY exceptions: 3pt\n",
    "2. Perfect 1:1 reproducibility on those machines: 1pt\n",
    "3. Very clear github-repo-style README.MD with instructions for running the code: 1pt\n",
    "4. Report: 1pt\n",
    "5. Model test performance on test-set: interpolated from 30-80% test-accuracy: 0-3pt\n",
    "6. Pick 10 random pictures of the test set to simulate a user uploading images and report which categories occur how often in these: 1pt\n",
    "7. Bonus point: use an LLM (API) to generate short description / profile of preferences of the simulated user\n",
    "\n",
    "**The main reason why we mention the machines in the uni computer rooms is to make it clear to you how we will test your submissions and to make you aware that you can use these machines for working on your assignment if you do not have access to a computer with enough gpu. You do not have to use these machines for this assignment. Using the specified pytorch version, not enabling torch.backends.cuda.matmul.allow_tf32, setting fixed seeds and making sure your code doesn't use more than 8gb VRAM should allow you to fulfill the reproducibility criteria.**\n",
    "\n",
    "**(If there is anything unclear about this assignment please post your question in the Brightspace discussions forum or send an email)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52be6aa80281ab2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T15:02:07.839216Z",
     "start_time": "2025-05-01T15:02:07.835872Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc8f550a31afdd4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T15:02:09.788508Z",
     "start_time": "2025-05-01T15:02:09.779381Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "\n",
    "# Ensure deterministic behaviour on GPU\n",
    "torch.backends.cuda.matmul.allow_tf32 = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Enable autotuner for faster convolutions\n",
    "torch.backends.cudnn.benchmark = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c222e5c4ff1b26",
   "metadata": {},
   "source": [
    "# Loading the datasets\n",
    "The dataset is already split into a train and test set in the directories \"train\" and \"test\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187b372df2dcc23b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T15:02:57.509055Z",
     "start_time": "2025-05-01T15:02:12.133603Z"
    }
   },
   "outputs": [],
   "source": [
    "# FoodDataset reads the folder structure and returns (tensor, label)\n",
    "class FoodDataset(Dataset):\n",
    "    def __init__(self, root_dir, encoder, transform=None):\n",
    "        self.images, self.labels = [], []\n",
    "        self.transform, self.encoder = transform, encoder\n",
    "\n",
    "        # collect all image paths + string labels\n",
    "        str_labels = []\n",
    "        for folder in os.scandir(root_dir):\n",
    "            if not folder.is_dir():\n",
    "                continue\n",
    "            for img in os.scandir(folder):\n",
    "                str_labels.append(folder.name)\n",
    "                self.images.append(str(img.path))\n",
    "\n",
    "        # encode strings → ints\n",
    "        self.labels = self.encoder.transform(str_labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.images[idx]).convert('RGB') # use RGB\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, self.labels[idx]\n",
    "\n",
    "    def get_norm_stats(self, size=(150, 150)):\n",
    "        \"\"\"Compute per‑channel mean & std over the whole dataset.\"\"\"\n",
    "        s1, s2, n = np.zeros(3), np.zeros(3), 0\n",
    "        for p in self.images:\n",
    "            arr = np.array(Image.open(p).convert('RGB').resize(size), np.float32) / 255\n",
    "            s1 += arr.sum((0, 1))\n",
    "            s2 += (arr ** 2).sum((0, 1))\n",
    "            n += arr.shape[0] * arr.shape[1]\n",
    "        mean = s1 / n\n",
    "        std = np.sqrt(s2 / n - mean ** 2)\n",
    "        return mean.tolist(), std.tolist()\n",
    "\n",
    "    def set_transform(self, tf):\n",
    "        self.transform = tf\n",
    "\n",
    "    # get list of all class names\n",
    "class_names = [d.name for d in os.scandir(\"train\") if d.is_dir()]\n",
    "encoder = LabelEncoder().fit(class_names)\n",
    "\n",
    "train_ds = FoodDataset(\"train\", encoder)\n",
    "test_ds  = FoodDataset(\"test\",  encoder)\n",
    "\n",
    "train_mean, train_std = train_ds.get_norm_stats()\n",
    "test_mean , test_std  = test_ds.get_norm_stats()\n",
    "\n",
    "# training augmentations\n",
    "train_tf = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((160, 160)), # resize to 160x160\n",
    "    torchvision.transforms.RandomCrop(150), # crop back to 150x150\n",
    "    torchvision.transforms.RandomHorizontalFlip(), # random horizontal flip\n",
    "    torchvision.transforms.RandomRotation(15),  # random rotation\n",
    "    torchvision.transforms.ColorJitter(0.2, 0.2, 0.2), # brightness, contrast, saturation\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.RandomErasing(p=0.5), # random erasing\n",
    "    torchvision.transforms.Normalize(train_mean, train_std) # normalize to mean=0, std=1\n",
    "])\n",
    "# test transforms\n",
    "test_tf = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((150, 150)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(test_mean, test_std)\n",
    "])\n",
    "\n",
    "train_ds.set_transform(train_tf)\n",
    "test_ds.set_transform(test_tf)\n",
    "\n",
    "# num_workers=0 avoids hanging in a Windows notebook (was needed in PyTorch 1.7.0)\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True,  num_workers=0)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=32, shuffle=False, num_workers=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7bb30ae14ffa42",
   "metadata": {},
   "source": [
    "# CNN Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdb0d003d6df777",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T15:03:12.160703Z",
     "start_time": "2025-05-01T15:03:12.153939Z"
    }
   },
   "outputs": [],
   "source": [
    "class FoodCNN(nn.Module):\n",
    "    \"\"\"Uses 3 convolutional blocks with 2 conv layers each, followed by a linear classifier.\"\"\"\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            # block 1: 3→32\n",
    "            nn.Conv2d(3, 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                       # 32×150×150 → 32×75×75\n",
    "\n",
    "            # block 2: 32→64\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                       # 64×75×75 → 64×37×37\n",
    "\n",
    "            # block 3: 64→128\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(),\n",
    "            nn.MaxPool2d(2)                       # 128×37×37 → 128×18×18\n",
    "        )\n",
    "        # flatten size computed with dummy tensor\n",
    "        dummy = torch.zeros(1, 3, 150, 150)\n",
    "        flat = self.features(dummy).numel()\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(flat, 128), nn.BatchNorm1d(128), nn.ReLU(), nn.Dropout(0.1),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(self.features(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c602d154e795a27",
   "metadata": {},
   "source": [
    "# Training the model\n",
    "Implement your training process below. Report the test-accuracy after every epoch for the training run of the final model.\n",
    "\n",
    "Hint: before training your model make sure to reset the seed in the training cell, as otherwise the seed may have changed due to previous training runs in the notebook\n",
    "\n",
    "Note: If you implement automatic hyperparameter tuning, split the train set into train and validation subsets for the objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9eb5c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T15:03:14.855985Z",
     "start_time": "2025-05-01T15:03:14.846518Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu126 12.6\n",
      "True\n",
      "NVIDIA GeForce GTX 1050 Ti\n"
     ]
    }
   ],
   "source": [
    "# check if GPU is available and use it if possible, run on CPU otherwise\n",
    "\n",
    "print(torch.__version__, torch.version.cuda)\n",
    "print(torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff7d9d84c06f5d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T15:17:47.966920Z",
     "start_time": "2025-05-01T15:03:16.710677Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/20 | loss 4.1197 | acc 11.61%\n",
      "Epoch 02/20 | loss 3.7468 | acc 17.16%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 39\u001b[0m\n\u001b[0;32m     37\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     38\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m---> 39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     40\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     42\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\pepij\\anaconda3\\envs\\foodcnn\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    714\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\pepij\\anaconda3\\envs\\foodcnn\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\pepij\\anaconda3\\envs\\foodcnn\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[7], line 25\u001b[0m, in \u001b[0;36mFoodDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     23\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages[idx])\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m---> 25\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[idx]\n",
      "File \u001b[1;32mc:\\Users\\pepij\\anaconda3\\envs\\foodcnn\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\pepij\\anaconda3\\envs\\foodcnn\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pepij\\anaconda3\\envs\\foodcnn\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\pepij\\anaconda3\\envs\\foodcnn\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:354\u001b[0m, in \u001b[0;36mResize.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m    347\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;124;03m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;124;03m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pepij\\anaconda3\\envs\\foodcnn\\Lib\\site-packages\\torchvision\\transforms\\functional.py:477\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[0;32m    475\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    476\u001b[0m     pil_interpolation \u001b[38;5;241m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[1;32m--> 477\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpil_interpolation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39mresize(img, size\u001b[38;5;241m=\u001b[39moutput_size, interpolation\u001b[38;5;241m=\u001b[39minterpolation\u001b[38;5;241m.\u001b[39mvalue, antialias\u001b[38;5;241m=\u001b[39mantialias)\n",
      "File \u001b[1;32mc:\\Users\\pepij\\anaconda3\\envs\\foodcnn\\Lib\\site-packages\\torchvision\\transforms\\_functional_pil.py:250\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(size, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m):\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot inappropriate size arg: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pepij\\anaconda3\\envs\\foodcnn\\Lib\\site-packages\\PIL\\Image.py:2356\u001b[0m, in \u001b[0;36mImage.resize\u001b[1;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[0;32m   2344\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2345\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduce(factor, box\u001b[38;5;241m=\u001b[39mreduce_box)\n\u001b[0;32m   2346\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduce)\n\u001b[0;32m   2347\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m Image\u001b[38;5;241m.\u001b[39mreduce(\u001b[38;5;28mself\u001b[39m, factor, box\u001b[38;5;241m=\u001b[39mreduce_box)\n\u001b[0;32m   2348\u001b[0m         )\n\u001b[0;32m   2349\u001b[0m         box \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2350\u001b[0m             (box[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[0;32m   2351\u001b[0m             (box[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[0;32m   2352\u001b[0m             (box[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[0;32m   2353\u001b[0m             (box[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[0;32m   2354\u001b[0m         )\n\u001b[1;32m-> 2356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# reset the seed in this cell (important for reruns)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "\n",
    "def accuracy(logits, labels):\n",
    "    \"\"\"Return percentage of correct predictions in a batch.\"\"\"\n",
    "    return (logits.argmax(1) == labels).float().mean().item() * 100\n",
    "\n",
    "@torch.no_grad()            # no gradients during evaluation, saves memory\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    acc_sum, n = 0.0, 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        acc_sum += accuracy(model(x), y) * x.size(0)\n",
    "        n += x.size(0)\n",
    "    return acc_sum / n\n",
    "\n",
    "# hyper‑parameters\n",
    "batch_size    = 32\n",
    "learning_rate = 1e-3 # initial learning rate, decreases during training by scheduler\n",
    "num_epochs    = 100\n",
    "num_classes   = len(class_names)\n",
    "\n",
    "# build model\n",
    "model = FoodCNN(num_classes).to(device)\n",
    "\n",
    "criterion  = nn.CrossEntropyLoss()\n",
    "optimizer  = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "scheduler  = optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=1e-2,\n",
    "    total_steps=num_epochs * len(train_loader)\n",
    ")\n",
    "\n",
    "best = 0.0\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model(x), y) \n",
    "        loss.backward() \n",
    "        optimizer.step()\n",
    "        scheduler.step()          \n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    val_acc = evaluate(model, test_loader)\n",
    "    print(f\"Epoch {epoch:02d}/{num_epochs} | \"\n",
    "          f\"loss {epoch_loss/len(train_loader):.4f} | acc {val_acc:.2f}%\")\n",
    "\n",
    "    if val_acc > best:\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "        best = val_acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb476e18bd30968c",
   "metadata": {},
   "source": [
    "# Calculating model performance\n",
    "Load the best version of your model ( which should be produced and saved by previous cells ), calculate and report the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eaa35096547d04",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T15:18:15.465679Z",
     "start_time": "2025-05-01T15:17:51.165873Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Accuracy: 6.92%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "print(\"Final test accuracy:\", evaluate(model, test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ecc6f7f921591e",
   "metadata": {},
   "source": [
    "# Summary of hyperparameters\n",
    "Report the hyperparameters ( learning rate etc ) that you used in your final model for reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b6a524e28b431a",
   "metadata": {},
   "source": [
    "# Simulation of random user\n",
    "Pick 10 random pictures of the test set to simulate a user uploading images and report which categories occur how often in these: 1pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6e8175cacc8dfe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T15:18:20.328233Z",
     "start_time": "2025-05-01T15:18:20.299355Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted class frequencies: {np.str_('beignets'): 1, np.str_('panna_cotta'): 1, np.str_('hot_dog'): 1, np.str_('lobster_bisque'): 1, np.str_('ravioli'): 1, np.str_('prime_rib'): 1, np.str_('risotto'): 1, np.str_('seaweed_salad'): 1, np.str_('eggs_benedict'): 1, np.str_('mussels'): 1}\n",
      "actual class frequencies: {'test\\\\hot_dog': 1, 'test\\\\chicken_curry': 1, 'test\\\\pho': 1, 'test\\\\clam_chowder': 1, 'test\\\\sashimi': 1, 'test\\\\poutine': 1, 'test\\\\spaghetti_carbonara': 1, 'test\\\\creme_brulee': 1, 'test\\\\club_sandwich': 1, 'test\\\\beet_salad': 1}\n"
     ]
    }
   ],
   "source": [
    "def img_to_tensor(img, mean, std):\n",
    "    \"\"\"\n",
    "    makes image tensor with necessary transformations\n",
    "    :param img: path to image\n",
    "    :param mean: mean from image parent folder (probably test_mean)\n",
    "    :param std: standard deviation from folder (probably test_std)\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # transformation pipeline\n",
    "    tf = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize((150, 150)),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(mean, std)])\n",
    "    return tf(Image.open(img_path).convert('RGB')) ## Max what did you have for img_path? :))\n",
    "\n",
    "def get_random_images(folder, encoder, mean, std, device, n=10):\n",
    "    \"\"\"Pick n random test images and return (true_classes, predicted_classes).\"\"\"\n",
    "    imgs, true_cls = [], []\n",
    "    tf = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize((150, 150)),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(mean, std)])\n",
    "    for i in range(n):\n",
    "        cls = np.random.choice([d for d in os.listdir(folder)\n",
    "                             if os.path.isdir(os.path.join(folder, d))])\n",
    "        fp = np.random.choice(os.listdir(os.path.join(folder, cls)))\n",
    "        path = os.path.join(folder, cls, fp)\n",
    "        imgs.append(tf(Image.open(path).convert('RGB')))\n",
    "        true_cls.append(cls)\n",
    "\n",
    "    batch = torch.stack(imgs).to(device)\n",
    "    preds = model(batch).argmax(1).cpu().numpy()\n",
    "    pred_cls = encoder.inverse_transform(preds)\n",
    "    return true_cls, pred_cls\n",
    "\n",
    "def get_predictions(model, images, encoder):\n",
    "    images = images.to(device) # put image into gpu (cuda) if available\n",
    "    predictions = model(images).argmax(dim=1) # get model predictions\n",
    "    string_predictions = list(encoder.inverse_transform(predictions.flatten().tolist())) # decode predicted labels into string classes\n",
    "    return string_predictions\n",
    "\n",
    "def make_frequency_dict(classes):\n",
    "    freq_dict = {}\n",
    "    for c in classes:\n",
    "        if c not in freq_dict:\n",
    "            freq_dict[c] = 1\n",
    "        else:\n",
    "            freq_dict[c] += 1\n",
    "    return freq_dict\n",
    "\n",
    "images, random_classes = get_random_images('test')\n",
    "predictions = get_predictions(model, images, encoder)\n",
    "actual_frequencies = make_frequency_dict(random_classes)\n",
    "pred_frequencies = make_frequency_dict(predictions)\n",
    "\n",
    "print(f\"predicted class frequencies: {pred_frequencies}\")\n",
    "print(f\"actual class frequencies: {actual_frequencies}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e7a3634bf6861f",
   "metadata": {},
   "source": [
    "# Bonus point\n",
    "Use an LLM (API) to generate a description of the food preference of a user based on 10 images that a potential user could provide. \n",
    "Please include an example of the output of your code, especially if you used an API other than the OpenAI API.\n",
    "\n",
    "This should work well even with differing test images by setting different random seeds for the image selector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6819fa0042485dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"Use openai API, give him frequency dict and ask to generate a text profile of the user based on the frequencies\"\"\"\n",
    "\n",
    "import os, json, openai\n",
    "from collections import Counter\n",
    "\n",
    "openai.api_key = os.getenv(\"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\")  # put your key in env var\n",
    "freq = Counter(pred_cls)           \n",
    "\n",
    "# prompt\n",
    "sys_msg  = \"You are a food critic who writes short taste profiles.\"\n",
    "user_msg = (\"Based on these dish counts, describe the user's food preferences in 3–4 lines.\\n\"\n",
    "    f\"Dish counts (label : times liked):\\n{json.dumps(freq, indent=2)}\")\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[{\"role\":\"system\",\"content\":sys_msg},\n",
    "              {\"role\":\"user\",\"content\":user_msg}],\n",
    "    temperature=0.7 # creativity\n",
    ")\n",
    "profile_text = response.choices[0].message.content.strip()\n",
    "print(profile_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foodcnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
