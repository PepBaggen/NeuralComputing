{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf1be1b2f55b62c0",
   "metadata": {},
   "source": [
    "# Food Classification with CNN - Building a Restaurant Recommendation System\n",
    "\n",
    "This assignment focuses on developing a deep learning-based food classification system using Convolutional Neural Networks (CNNs). You will build a model that can recognize different food categories and use it to return the food preferences of a user.\n",
    "\n",
    "## Learning Objectives\n",
    "- Implement CNNs for image classification\n",
    "- Work with real-world food image datasets\n",
    "- Build a preference-detector system\n",
    "\n",
    "## Background: AI-Powered Food Preference Discovery\n",
    "\n",
    "The system's core idea is simple:\n",
    "\n",
    "1. Users upload 10 photos of dishes they enjoy\n",
    "2. Your CNN classifies these images into the 91 categories\n",
    "3. Based on these categories, the system returns the user's taste profile\n",
    "\n",
    "Your task is to develop the core computer vision component that will power this detection engine.\n",
    "\n",
    "You are given a training (\"train\" folder) and a test (\"test\" folder) dataset which have ~45k and ~22k samples respectively. For each one of the 91 classes there is a subdirectory containing the images of the respective class.\n",
    "\n",
    "## Assignment Requirements\n",
    "\n",
    "### Technical Requirements\n",
    "- Implement your own pytorch CNN architecture for food image classification\n",
    "- Use only the provided training dataset split for training\n",
    "- Train the network from scratch ; No pretrained weights can be used\n",
    "- Report test-accuracy after every epoch\n",
    "- Report all hyperparameters of final model\n",
    "- Use a fixed seed and do not use any CUDA-features that break reproducibility\n",
    "- Use Pytorch 2.6\n",
    "\n",
    "### Deliverables\n",
    "1. Jupyter Notebook with CNN implementation, training code etc.\n",
    "2. README file\n",
    "3. Report (max 3 pages)\n",
    "\n",
    "Submit your report, README and all code files as a single zip file named GROUP_[number]_NC2425_PA. The names and IDs of the group components must be mentioned in the README.\n",
    "Do not include the dataset in your submission.\n",
    "\n",
    "### Grading\n",
    "\n",
    "1. Correct CNN implementation, training runs on the uni computers (computer rooms DM.0.07, DM.0.13, DM.0.17, DM.0.21) according to the README.MD instructions without ANY exceptions: 3pt\n",
    "2. Perfect 1:1 reproducibility on those machines: 1pt\n",
    "3. Very clear github-repo-style README.MD with instructions for running the code: 1pt\n",
    "4. Report: 1pt\n",
    "5. Model test performance on test-set: interpolated from 30-80% test-accuracy: 0-3pt\n",
    "6. Pick 10 random pictures of the test set to simulate a user uploading images and report which categories occur how often in these: 1pt\n",
    "7. Bonus point: use an LLM (API) to generate short description / profile of preferences of the simulated user\n",
    "\n",
    "**The main reason why we mention the machines in the uni computer rooms is to make it clear to you how we will test your submissions and to make you aware that you can use these machines for working on your assignment if you do not have access to a computer with enough gpu. You do not have to use these machines for this assignment. Using the specified pytorch version, not enabling torch.backends.cuda.matmul.allow_tf32, setting fixed seeds and making sure your code doesn't use more than 8gb VRAM should allow you to fulfill the reproducibility criteria.**\n",
    "\n",
    "**(If there is anything unclear about this assignment please post your question in the Brightspace discussions forum or send an email)**\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "52be6aa80281ab2b",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-01T15:02:07.839216Z",
     "start_time": "2025-05-01T15:02:07.835872Z"
    }
   },
   "source": [
    "import pandas\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n"
   ],
   "outputs": [],
   "execution_count": 52
  },
  {
   "cell_type": "code",
   "id": "2dc8f550a31afdd4",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-01T15:02:09.788508Z",
     "start_time": "2025-05-01T15:02:09.779381Z"
    }
   },
   "source": [
    "#os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"   # deterministic mode for GPU\n",
    "\n",
    "random_seed = 42\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n"
   ],
   "outputs": [],
   "execution_count": 53
  },
  {
   "cell_type": "markdown",
   "id": "74c222e5c4ff1b26",
   "metadata": {},
   "source": [
    "# Loading the datasets\n",
    "The dataset is already split into a train and test set in the directories \"train\" and \"test\"."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T15:02:57.509055Z",
     "start_time": "2025-05-01T15:02:12.133603Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# different dataloader approach to allow for data augmentation\n",
    "\n",
    "class FoodDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, encoder=None):\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.transform = transform\n",
    "        self.encoder = encoder\n",
    "\n",
    "        string_labels = []\n",
    "        for folder in os.scandir(root_dir): # iterate through the contents of the directory\n",
    "            if not folder.is_dir(): # make sure content is directory (could be .DS_Store file)\n",
    "                continue\n",
    "            for image in os.scandir(folder):\n",
    "                string_labels.append(folder.name) # an image's parent folder name is its label\n",
    "                self.images.append(image)\n",
    "        self.labels = self.encoder.transform(string_labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.images[idx])\n",
    "        image = ImageOps.grayscale(image)\n",
    "        label = self.labels[idx]\n",
    "        if self.transform: # make transformations\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "    def get_normalization_values(self):\n",
    "        \"\"\"\n",
    "        calculate the mean and standard deviation of the image data to normalize it\n",
    "        :return: mean and standard deviation\n",
    "        \"\"\"\n",
    "        pixel_sum = 0.0 # for rgb: np.zeros(3)\n",
    "        pixel_squared_sum = 0.0 # for rgb: np.zeros(3)\n",
    "        pixel_count = 0\n",
    "        for image in self.images:\n",
    "            img = Image.open(image).resize((64, 64))\n",
    "            img = ImageOps.grayscale(img)  # Either remove or use convert RGB\n",
    "            img = np.array(img, dtype=np.float32) / 255.0  # shape: (H, W)\n",
    "            pixel_sum += img.sum()\n",
    "            pixel_squared_sum += (img ** 2).sum() # add axis (0,1) or something\n",
    "            pixel_count += img.size # shape[0] * shape[1]\n",
    "\n",
    "        mean = pixel_sum / pixel_count\n",
    "        std = np.sqrt(pixel_squared_sum / pixel_count - mean ** 2)\n",
    "        return mean, std\n",
    "\n",
    "    def set_transform(self, transform):\n",
    "        \"\"\"\n",
    "        sets transformation pipeline\n",
    "        :param transform: transformation pipeline\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.transform = transform\n",
    "\n",
    "# get list of all class names\n",
    "class_names = []\n",
    "for folder in os.scandir(\"train\"):\n",
    "    if folder.is_dir():\n",
    "        class_names.append(folder.name)\n",
    "\n",
    "# encode class strings into integer tensors\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(class_names)\n",
    "labels = encoder.fit_transform(class_names)\n",
    "\n",
    "print(\"making train_dataset\")\n",
    "train_dataset = FoodDataset(root_dir=\"train\", encoder=encoder)\n",
    "print(\"making test_dataset\\n\")\n",
    "test_dataset  = FoodDataset(root_dir=\"test\", encoder=encoder)\n",
    "\n",
    "print(\"calculating train normalization values\")\n",
    "train_mean, train_std = train_dataset.get_normalization_values()\n",
    "print(f\"mean = {train_mean}, std = {train_std}\\n\")\n",
    "\n",
    "# transformations pipeline for train dataset\n",
    "train_transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((64, 64)), # resize\n",
    "    torchvision.transforms.RandomHorizontalFlip(), # chance to randomly flip image\n",
    "    torchvision.transforms.RandomRotation(15), # chance to randomly rotate\n",
    "    torchvision.transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2), # randomly apply color changes\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(train_mean, train_std) # normalize according to mean and std\n",
    "])\n",
    "\n",
    "print(\"calculating test normalization values:\")\n",
    "test_mean, test_std = test_dataset.get_normalization_values()\n",
    "print(f\"mean = {train_mean}, std = {train_std}\\n\")\n",
    "\n",
    "# transformation pipeline for test dataset (no random augmentations here)\n",
    "test_transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((64, 64)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(test_mean, test_std)\n",
    "])\n",
    "\n",
    "print(\"setting transforms\")\n",
    "train_dataset.set_transform(train_transforms)\n",
    "test_dataset.set_transform(test_transforms)\n",
    "\n",
    "print(\"making dataloaders\")\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "print(\"done\")\n",
    "\n",
    "# test dataset\n",
    "x, y = next(iter(train_loader))\n",
    "print(\"Input shape:\", x.shape)  # Expect: torch.Size([16, 1, 224, 224])\n",
    "print(\"Input range:\", x.min().item(), \"to\", x.max().item())  # Expect: ~-2 to 2\n",
    "print(\"Labels:\", y[:10])  # Expect: Tensor of class indices between 0 and 90\n",
    "print(\"Label dtype:\", y.dtype)  # Expect: torch.int64"
   ],
   "id": "187b372df2dcc23b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making train_dataset\n",
      "making test_dataset\n",
      "\n",
      "calculating train normalization values\n",
      "mean = 0.46283867955207825, std = 0.2549550235271454\n",
      "\n",
      "calculating test normalization values:\n",
      "mean = 0.46283867955207825, std = 0.2549550235271454\n",
      "\n",
      "setting transforms\n",
      "making dataloaders\n",
      "done\n",
      "Input shape: torch.Size([16, 1, 64, 64])\n",
      "Input range: -1.8153737783432007 to 1.9992165565490723\n",
      "Labels: tensor([42, 50, 88, 81, 81, 56, 61, 17,  2, 48])\n",
      "Label dtype: torch.int64\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T15:03:01.323721Z",
     "start_time": "2025-05-01T15:03:01.305318Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# small scale dataset for test\n",
    "# only run if you want it (not recommended)\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "subset = Subset(train_dataset, list(range(32)))\n",
    "tiny_loader = DataLoader(subset, batch_size=8, shuffle=True)"
   ],
   "id": "c06398c5038e062d",
   "outputs": [],
   "execution_count": 55
  },
  {
   "cell_type": "markdown",
   "id": "2f7bb30ae14ffa42",
   "metadata": {},
   "source": [
    "# CNN Implementation"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T15:03:12.160703Z",
     "start_time": "2025-05-01T15:03:12.153939Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# original cnn\n",
    "def get_num_classes(f):\n",
    "    class_count = 0\n",
    "    for folder in os.scandir(f): # iterate through the contents of the directory\n",
    "        if not folder.is_dir(): # make sure content is directory (could be .DS_Store file)\n",
    "            continue\n",
    "        class_count += 1\n",
    "    return class_count\n",
    "batch_size = 16 # 16: safe, 32: faster but might not run on every pc\n",
    "num_classes = get_num_classes(\"train\")\n",
    "learning_rate = 0.001\n",
    "num_epochs = 20\n",
    "\n",
    "class FoodCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, padding=1),  # first argument 1 for gray 3 for rgb\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 32, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),                 # 3×224×224 → 32×112×112\n",
    "\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),                 # 64×56×56\n",
    "        )\n",
    "\n",
    "        # compute flattened size using dummy tensor with only 0s\n",
    "        dummy = torch.zeros(1, 1, 64, 64) # second argument 1 for gray 3 for rgb\n",
    "        flat = self.features(dummy).numel()\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(flat, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3), # randomly ignores part of the neurons during train (not eval) in order to prevent overfitting -- can be removed or changed\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ],
   "id": "2fdb0d003d6df777",
   "outputs": [],
   "execution_count": 58
  },
  {
   "cell_type": "markdown",
   "id": "8c602d154e795a27",
   "metadata": {},
   "source": [
    "# Training the model\n",
    "Implement your training process below. Report the test-accuracy after every epoch for the training run of the final model.\n",
    "\n",
    "Hint: before training your model make sure to reset the seed in the training cell, as otherwise the seed may have changed due to previous training runs in the notebook\n",
    "\n",
    "Note: If you implement automatic hyperparameter tuning, split the train set into train and validation subsets for the objective function."
   ]
  },
  {
   "cell_type": "code",
   "id": "7d9eb5c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T15:03:14.855985Z",
     "start_time": "2025-05-01T15:03:14.846518Z"
    }
   },
   "source": [
    "# check if GPU is available and use it if possible, run on CPU otherwise\n",
    "\n",
    "import torch\n",
    "print(torch.__version__, torch.version.cuda)\n",
    "print(torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0+cu128 12.8\n",
      "True\n",
      "NVIDIA GeForce RTX 4070 Laptop GPU\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "cell_type": "code",
   "id": "7ff7d9d84c06f5d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T15:17:47.966920Z",
     "start_time": "2025-05-01T15:03:16.710677Z"
    }
   },
   "source": [
    "     # reset the seed in this cell\n",
    "def accuracy(logits, labels):\n",
    "    preds = logits.argmax(dim=1)\n",
    "    return (preds == labels).float().mean().item() * 100\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval() # sets model into eval mode (disables dropout and weight change etc)\n",
    "    acc_sum, n = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader: # for each batch\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            acc_sum += accuracy(model(x), y) * x.size(0)\n",
    "            n += x.size(0)\n",
    "\n",
    "            # show first five predictions for each batch\n",
    "            # preds = model(x).argmax(dim=1)\n",
    "            # print(\"Sample predictions:\", preds[:5].tolist())\n",
    "            # print(\"As labels:\", encoder.inverse_transform(preds[:5].tolist()))\n",
    "\n",
    "    return acc_sum / n\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # use gpu if available\n",
    "model = FoodCNN(num_classes).to(device)\n",
    "\n",
    "criterion  = nn.CrossEntropyLoss()\n",
    "# optimizer  = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=0.005, momentum = 0.9) # optimizer from practical classes\n",
    "#scheduler  = optim.lr_scheduler.StepLR(optimizer, step_size=8, gamma=0.1)\n",
    "\n",
    "best = 0.0\n",
    "for epoch in range(1, num_epochs):\n",
    "    model.train() # set model into training mode (enable weight change, dropout etc)\n",
    "    print(\"training epoch:\", epoch)\n",
    "    running_loss = 0.0\n",
    "    for x, y in train_loader: # for each batch\n",
    "\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        # training steps i think\n",
    "        loss = criterion(model(x), y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    test_acc = evaluate(model, test_loader)\n",
    "    print(f\"epoch {epoch:02d} | test acc {test_acc:.2f}%\")\n",
    "    print(f\"Epoch {epoch:02d} | Train loss: {running_loss:.4f}\")\n",
    "\n",
    "    if test_acc > best:\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "        best = test_acc\n",
    "\n",
    "    #scheduler.step()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch: 1\n",
      "epoch 01 | test acc 2.60%\n",
      "Epoch 01 | Train loss: 12822.3368\n",
      "training epoch: 2\n",
      "epoch 02 | test acc 3.39%\n",
      "Epoch 02 | Train loss: 12593.6609\n",
      "training epoch: 3\n",
      "epoch 03 | test acc 4.11%\n",
      "Epoch 03 | Train loss: 12466.8840\n",
      "training epoch: 4\n",
      "epoch 04 | test acc 4.08%\n",
      "Epoch 04 | Train loss: 12367.4563\n",
      "training epoch: 5\n",
      "epoch 05 | test acc 5.03%\n",
      "Epoch 05 | Train loss: 12292.2072\n",
      "training epoch: 6\n",
      "epoch 06 | test acc 6.03%\n",
      "Epoch 06 | Train loss: 12189.0708\n",
      "training epoch: 7\n",
      "epoch 07 | test acc 6.63%\n",
      "Epoch 07 | Train loss: 12069.4848\n",
      "training epoch: 8\n",
      "epoch 08 | test acc 6.92%\n",
      "Epoch 08 | Train loss: 11984.1359\n",
      "training epoch: 9\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[60]\u001B[39m\u001B[32m, line 47\u001B[39m\n\u001B[32m     44\u001B[39m     optimizer.step()\n\u001B[32m     45\u001B[39m     running_loss += loss.item()\n\u001B[32m---> \u001B[39m\u001B[32m47\u001B[39m test_acc = \u001B[43mevaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_loader\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     48\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mepoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m02d\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m | test acc \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtest_acc\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.2f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m%\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     49\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m02d\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m | Train loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrunning_loss\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[60]\u001B[39m\u001B[32m, line 10\u001B[39m, in \u001B[36mevaluate\u001B[39m\u001B[34m(model, loader)\u001B[39m\n\u001B[32m      8\u001B[39m acc_sum, n = \u001B[32m0.0\u001B[39m, \u001B[32m0\u001B[39m\n\u001B[32m      9\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m torch.no_grad():\n\u001B[32m---> \u001B[39m\u001B[32m10\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mloader\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;66;43;03m# for each batch\u001B[39;49;00m\n\u001B[32m     11\u001B[39m \u001B[43m        \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     12\u001B[39m \u001B[43m        \u001B[49m\u001B[43macc_sum\u001B[49m\u001B[43m \u001B[49m\u001B[43m+\u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43maccuracy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m.\u001B[49m\u001B[43msize\u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\LeidenDSAI\\semester4\\neural_computing\\NC_assignment_2425\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001B[39m, in \u001B[36m_BaseDataLoaderIter.__next__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    730\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    731\u001B[39m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[32m    732\u001B[39m     \u001B[38;5;28mself\u001B[39m._reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m733\u001B[39m data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    734\u001B[39m \u001B[38;5;28mself\u001B[39m._num_yielded += \u001B[32m1\u001B[39m\n\u001B[32m    735\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m    736\u001B[39m     \u001B[38;5;28mself\u001B[39m._dataset_kind == _DatasetKind.Iterable\n\u001B[32m    737\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    738\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._num_yielded > \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called\n\u001B[32m    739\u001B[39m ):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\LeidenDSAI\\semester4\\neural_computing\\NC_assignment_2425\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001B[39m, in \u001B[36m_SingleProcessDataLoaderIter._next_data\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    787\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    788\u001B[39m     index = \u001B[38;5;28mself\u001B[39m._next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m789\u001B[39m     data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[32m    790\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._pin_memory:\n\u001B[32m    791\u001B[39m         data = _utils.pin_memory.pin_memory(data, \u001B[38;5;28mself\u001B[39m._pin_memory_device)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\LeidenDSAI\\semester4\\neural_computing\\NC_assignment_2425\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001B[39m, in \u001B[36m_MapDatasetFetcher.fetch\u001B[39m\u001B[34m(self, possibly_batched_index)\u001B[39m\n\u001B[32m     50\u001B[39m         data = \u001B[38;5;28mself\u001B[39m.dataset.__getitems__(possibly_batched_index)\n\u001B[32m     51\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m52\u001B[39m         data = [\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[32m     53\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m     54\u001B[39m     data = \u001B[38;5;28mself\u001B[39m.dataset[possibly_batched_index]\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[54]\u001B[39m\u001B[32m, line 27\u001B[39m, in \u001B[36mFoodDataset.__getitem__\u001B[39m\u001B[34m(self, idx)\u001B[39m\n\u001B[32m     25\u001B[39m label = \u001B[38;5;28mself\u001B[39m.labels[idx]\n\u001B[32m     26\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.transform: \u001B[38;5;66;03m# make transformations\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m27\u001B[39m     image = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     28\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m image, label\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\LeidenDSAI\\semester4\\neural_computing\\NC_assignment_2425\\.venv\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001B[39m, in \u001B[36mCompose.__call__\u001B[39m\u001B[34m(self, img)\u001B[39m\n\u001B[32m     93\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, img):\n\u001B[32m     94\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.transforms:\n\u001B[32m---> \u001B[39m\u001B[32m95\u001B[39m         img = \u001B[43mt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     96\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m img\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\LeidenDSAI\\semester4\\neural_computing\\NC_assignment_2425\\.venv\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001B[39m, in \u001B[36mToTensor.__call__\u001B[39m\u001B[34m(self, pic)\u001B[39m\n\u001B[32m    129\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, pic):\n\u001B[32m    130\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    131\u001B[39m \u001B[33;03m    Args:\u001B[39;00m\n\u001B[32m    132\u001B[39m \u001B[33;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    135\u001B[39m \u001B[33;03m        Tensor: Converted image.\u001B[39;00m\n\u001B[32m    136\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m137\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto_tensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpic\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\LeidenDSAI\\semester4\\neural_computing\\NC_assignment_2425\\.venv\\Lib\\site-packages\\torchvision\\transforms\\functional.py:176\u001B[39m, in \u001B[36mto_tensor\u001B[39m\u001B[34m(pic)\u001B[39m\n\u001B[32m    174\u001B[39m img = img.permute((\u001B[32m2\u001B[39m, \u001B[32m0\u001B[39m, \u001B[32m1\u001B[39m)).contiguous()\n\u001B[32m    175\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(img, torch.ByteTensor):\n\u001B[32m--> \u001B[39m\u001B[32m176\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mimg\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdefault_float_dtype\u001B[49m\u001B[43m)\u001B[49m.div(\u001B[32m255\u001B[39m)\n\u001B[32m    177\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    178\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m img\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "cell_type": "markdown",
   "id": "eb476e18bd30968c",
   "metadata": {},
   "source": [
    "# Calculating model performance\n",
    "Load the best version of your model ( which should be produced and saved by previous cells ), calculate and report the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "id": "1eaa35096547d04",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T15:18:15.465679Z",
     "start_time": "2025-05-01T15:17:51.165873Z"
    }
   },
   "source": [
    "# Load the best model weights\n",
    "model = FoodCNN(num_classes).to(device)\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "\n",
    "final_test_acc = evaluate(model, test_loader)\n",
    "print(f\"Final Test Accuracy: {final_test_acc:.2f}%\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Accuracy: 6.92%\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "cell_type": "markdown",
   "id": "85ecc6f7f921591e",
   "metadata": {},
   "source": [
    "# Summary of hyperparameters\n",
    "Report the hyperparameters ( learning rate etc ) that you used in your final model for reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b6a524e28b431a",
   "metadata": {},
   "source": [
    "# Simulation of random user\n",
    "Pick 10 random pictures of the test set to simulate a user uploading images and report which categories occur how often in these: 1pt"
   ]
  },
  {
   "cell_type": "code",
   "id": "2b6e8175cacc8dfe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T15:18:20.328233Z",
     "start_time": "2025-05-01T15:18:20.299355Z"
    }
   },
   "source": [
    "def img_to_tensor(img, mean, std):\n",
    "    \"\"\"\n",
    "    makes image tensor with necessary transformations\n",
    "    :param img: path to image\n",
    "    :param mean: mean from image parent folder (probably test_mean)\n",
    "    :param std: standard deviation from folder (probably test_std)\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # transformation pipeline\n",
    "    transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((64, 64)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean, std)\n",
    "    ])\n",
    "\n",
    "    # open and apply all transformations\n",
    "    image = Image.open(img)\n",
    "    image = ImageOps.grayscale(image)\n",
    "    image = transforms(image)\n",
    "    return image\n",
    "\n",
    "def get_random_images(folder):\n",
    "    \"\"\"\n",
    "    get 10 random images form a folder containing classes and images\n",
    "    :param folder: folder path (probably \"test\")\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    img_tensor_list = []\n",
    "    classes_list = []\n",
    "\n",
    "    for i in range(10):\n",
    "        random_class = None\n",
    "        while random_class is None:\n",
    "            folder_classes = os.listdir(folder) # list content of folder\n",
    "            random_class = folder_classes[np.random.randint(0, len(folder_classes) - 1)] # pick random class folder\n",
    "            if random_class == \".DS_Store\": random_class = None # .DS_Store can be inside the folders and is not a class\n",
    "        random_class_path = os.path.join(folder, random_class) # get path of chosen class folder\n",
    "        classes_list.append(random_class_path)\n",
    "\n",
    "        class_images = os.listdir(random_class_path) # get content of class\n",
    "        random_image = class_images[np.random.randint(0, len(class_images) - 1)] # pick random image\n",
    "        random_image_path = os.path.join(random_class_path, random_image) # get path\n",
    "\n",
    "        img_tensor = img_to_tensor(random_image_path, test_mean, test_std)\n",
    "        img_tensor_list.append(img_tensor)\n",
    "    all_img_tensor = torch.stack(img_tensor_list)\n",
    "    return all_img_tensor, classes_list\n",
    "\n",
    "def get_predictions(model, images, encoder):\n",
    "    images = images.to(device) # put image into gpu (cuda) if available\n",
    "    predictions = model(images).argmax(dim=1) # get model predictions\n",
    "    string_predictions = list(encoder.inverse_transform(predictions.flatten().tolist())) # decode predicted labels into string classes\n",
    "    return string_predictions\n",
    "\n",
    "def make_frequency_dict(classes):\n",
    "    freq_dict = {}\n",
    "    for c in classes:\n",
    "        if c not in freq_dict:\n",
    "            freq_dict[c] = 1\n",
    "        else:\n",
    "            freq_dict[c] += 1\n",
    "    return freq_dict\n",
    "\n",
    "images, random_classes = get_random_images('test')\n",
    "predictions = get_predictions(model, images, encoder)\n",
    "actual_frequencies = make_frequency_dict(random_classes)\n",
    "pred_frequencies = make_frequency_dict(predictions)\n",
    "\n",
    "print(f\"predicted class frequencies: {pred_frequencies}\")\n",
    "print(f\"actual class frequencies: {actual_frequencies}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted class frequencies: {np.str_('beignets'): 1, np.str_('panna_cotta'): 1, np.str_('hot_dog'): 1, np.str_('lobster_bisque'): 1, np.str_('ravioli'): 1, np.str_('prime_rib'): 1, np.str_('risotto'): 1, np.str_('seaweed_salad'): 1, np.str_('eggs_benedict'): 1, np.str_('mussels'): 1}\n",
      "actual class frequencies: {'test\\\\hot_dog': 1, 'test\\\\chicken_curry': 1, 'test\\\\pho': 1, 'test\\\\clam_chowder': 1, 'test\\\\sashimi': 1, 'test\\\\poutine': 1, 'test\\\\spaghetti_carbonara': 1, 'test\\\\creme_brulee': 1, 'test\\\\club_sandwich': 1, 'test\\\\beet_salad': 1}\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "cell_type": "markdown",
   "id": "88e7a3634bf6861f",
   "metadata": {},
   "source": [
    "# Bonus point\n",
    "Use an LLM (API) to generate a description of the food preference of a user based on 10 images that a potential user could provide. \n",
    "Please include an example of the output of your code, especially if you used an API other than the OpenAI API.\n",
    "\n",
    "This should work well even with differing test images by setting different random seeds for the image selector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6819fa0042485dae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foodcnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
